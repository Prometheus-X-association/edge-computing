# PTX-Edge Extension Testing

## Overview

Testings of the PTX-Edge extension on the Kubernetes ecosystem is
fundamentally performed using primarily the

- [kind](https://kind.sigs.k8s.io/) (vanilla K8s distribution) and
- [k3d](https://k3d.io/stable/) (lightweight K8s distribution for Edge/IoT) tools.

During the development of the `ptx-edge` K8s extension, testing tasks are conducted
on different levels described as follows.

- **Level 1**: Use a mock-up REST-API server based on an
  [OpenAPI specification](mock-api/swagger_server/swagger/swagger.yaml)
  as a standalone Docker container to validate feature endpoints and
  basic data required.

- **Level 2**: Use *kind/k3d* -- a Kubernetes test deployment (based on Docker-in-Docker)
  -- to initiate a test K8s environment with a mock-up REST-API instance
  with explicit port bindings.

- **Level 3**: Initiate a test K8s environment with a mock-up REST-API instance relying on
  internal K8s services, such as Services/LoadBalancer and Ingress/Gateway.

- **Level 4**: Create a DiD test Kubernetes setup, where the REST-API's endpoints
  are served by a standalone API component (based on its own autogenerated
  [openapi.json](../src/rest-api/spec/openapi.yaml)) initiated in an internal
  pod/deployment (DiD)

- **Level 5**: Create a DiD test Kubernetes setup with basic PTX-EDGE features installed,
  where REST-API entrypoints are served by a standalone API component.

- **Level 6**: Create test Kubernetes setup where the API endpoints are served by an internal
  component connected to the designed PTX K8s controller based on the same
  OpenAPI specification.

> [!IMPORTANT]
>
> The current setup creates a test environment based on the predefined **Level 4**,
> while the main [Makefile](../../Makefile) points to the *Level 3* setup!

Prior test level configs can be found in [levels](levels) folder.


> [!NOTE]
>
> In *Level 1-3*, the cluster creation method uses **kind**, whereas from *Level 4* the test
> cluster relies on the **k3d** tool!

## Preparation and Setup

### Level 1 testing (Mock API in single docker image)

To create a single docker container with the mockup REST-API,
use [levels/level1/Makefile](levels/level1/Makefile).

To create the image, use

```bash
$ make setup
```

To star the container,

```bash
$ make run
```

To remove the image, use

```bash
$ make cleanup
```

### Level 2 testing (Mock API in an emulated K8s cluster)

The test environment can be easily configured with the enclosed
[kubernetes/test/Makefile](levels/level3/Makefile).

To only install the test dependencies on an *Ubuntu 22.04/24.04* VM,
execute the following command in `kubernetes/test`:

```bash
$ make setup
```

while to also configure and set up the test environment, execute the following command:

```bash
$ make create
```

To launch the REST-API service in the test cluster, use

```bash
$ make run2
```

or the above steps combined:

```bash
$ make test2
```

To stop the service, use

```bash
$ make stop
```

To tear down the test environment, use

```bash
$ make cleanup
#or
$ make tear-down
```

and also to delete all intermediate resources, use

```bash
$ make purge
```

### Level 3 testing

To initiate a LoadBalancer for easy access to the exposed API port, use the following command variants:

```bash
$ make run3
$ make test3
```

To initiate a Nginx-based ingress controller and set up a default ingress rule(`/ptx-edge/v1/`), use the following
variants:

```bash
$ make run3-ingress
$ make test3-ingress
```

## Test Environment

The detailed description of the used K8s test environment(s) is described below.

### kind

> **kind** is a tool for running local vanilla Kubernetes clusters using Docker
> container “nodes”.
> kind was primarily designed for testing Kubernetes itself, but may be used for
> local development or CI.

The test environment can be set up locally based on Ubuntu 22.04/24.04 LTS by
cloning the repository and executing the following script.

```bash
$ git clone https://github.com/Prometheus-X-association/edge-computing.git
$ cd edge-computing/kubernetes/test
$ ./setup_kind_test_env.sh
```

The script installs the required dependencies

- [Docker](https://docs.docker.com/get-started/get-docker/) (latest)
- [kind](https://github.com/kubernetes-sigs/kind/releases/tag/v0.26.0) (v0.26.0)
- [kubectl](https://github.com/kubernetes/kubectl/releases/tag/v0.32.0) (v1.32.0)

and performs a simple test deployment on a temporary Kubernetes
cluster for validation.

The setup script also supports a rootless installation by using
the `-r` flag.

```bash
$ ./setup_kind_test_env.sh -r
```

See more about rootless mode and its limitations in
[here](https://docs.docker.com/engine/security/rootless/)
and [here](https://kind.sigs.k8s.io/docs/user/rootless/).

Other configuration parameters are

- disabling the test deployment of a basic container at the end of the setup script,

```bash
$ ./setup_kind_test_env.sh -x
```

- performing a minimal installation for production environments (e.g. no bash completions),

```bash
$ ./setup_kind_test_env.sh -s
```

- installing Cloud Provider KIND for emulating a load balancer service of a cloud provider.

```bash
$ ./setup_kind_test_env.sh -c
```

### k3d

> **k3d** is a small program made for running a *K3s* cluster in Docker.
> [K3s](https://k3s.io/) is a lightweight, CNCF-certified Kubernetes distribution and
> Sandbox project.
> k3d uses a Docker image built from the K3s repository to spin up multiple K3s nodes
> in Docker containers on any machine with Docker installed.

The script installs the required dependencies

- [Docker](https://docs.docker.com/get-started/get-docker/) (latest)
- [k3d](https://github.com/k3d-io/k3d/releases/tag/v5.8.2) (v5.8.2)
    - [k3s](https://github.com/k3s-io/k3s/releases/tag/v1.31.5%2Bk3s1)  (v1.31.5+k3s1)
- [kubectl](https://github.com/kubernetes/kubectl/releases/tag/v0.31.5) (v1.31.5)

To set up a k3d test environment, use the following script in `kubernetes/test`.

```bash
$ ./setup_k3d_test_env.sh
```

Other configuration parameters (as before) are

- disabling the test deployment of a basic container at the end of the setup script,

```bash
$ ./setup_k3d_test_env.sh -x
```

- performing a minimal installation for production environments (e.g. no bash completions),

```bash
$ ./setup_k3d_test_env.sh -s
```

## Extension Installation

TBD

## Tests

Test cases are composed of different unit test at different levels.

### K8s components/features

To test the feasibility of K8s features and manifest templates, use the following commands.

To install test dependencies with the latest versions:

```bash
$ cd suites && bash install-dep.sh
```

To execute the test cases, run the test scripts with the prefix `test-` in the `suite` folder.
For example:

```bash
$ ./test-policy-zone-scheduling.sh
```

> [!TIP]
>
> For optional parameters, refer to the
`shunit2` [GitHub documentation](https://github.com/kward/shunit2/blob/master/README.md).

To run all test script in the `suite` folder, use the following script:

```bash
$ ./runall.sh
```

#### Report Generation

The `runall` script can also generate JUnit-style reports about the test runs, in case the
report folder is defined using the flag `-o` (both absolute and relative path are allowed), e.g.,

```bash
$ ./runall.sh -o ./results
```

#### Examples

An example execution of one test case is the following:

```bash
$ ./test-policy-zone-scheduling.sh -- testPolicyZoneSchedulingWithNodeAffinity

################################################################################
###   Build images...
################################################################################
~/PTX-edge-computing/kubernetes/src/rest-api ~/PTX-edge-computing/kubernetes/test/suites
docker build -t ptx-edge/rest-api:1.0 .
[+] Building 1.4s (10/10) FINISHED                                                                                                  docker:default
 => [internal] load build definition from Dockerfile                                                                                          0.0s
 => => transferring dockerfile: 559B                                                                                                          0.0s
 => [internal] load metadata for docker.io/library/python:3.13-alpine                                                                         1.0s
 => [internal] load .dockerignore                                                                                                             0.0s
 => => transferring context: 121B                                                                                                             0.0s
 => [1/5] FROM docker.io/library/python:3.13-alpine@sha256:323a717dc4a010fee21e3f1aac738ee10bb485de4e7593ce242b36ee48d6b352                   0.0s
 => [internal] load build context                                                                                                             0.0s
 => => transferring context: 238B                                                                                                             0.0s
 => CACHED [2/5] WORKDIR /usr/src/api                                                                                                         0.0s
 => CACHED [3/5] COPY requirements.txt .                                                                                                      0.0s
 => CACHED [4/5] RUN python3 -m pip install --no-cache-dir -U -r requirements.txt                                                             0.0s
 => CACHED [5/5] COPY ./app ./app                                                                                                             0.0s
 => exporting to image                                                                                                                        0.0s
 => => exporting layers                                                                                                                       0.0s
 => => writing image sha256:9484d4ed4d4a3295a84067806dd8d0bfcde62545d5bbcb3255190f4e99bf01cb                                                  0.0s
 => => naming to docker.io/ptx-edge/rest-api:1.0                                                                                              0.0s
docker images --no-trunc ptx-edge/rest-api:1.0
REPOSITORY          TAG       IMAGE ID                                                                  CREATED        SIZE
ptx-edge/rest-api   1.0       sha256:9484d4ed4d4a3295a84067806dd8d0bfcde62545d5bbcb3255190f4e99bf01cb   45 hours ago   68MB
~/PTX-edge-computing/kubernetes/test/suites

################################################################################
###   Setup cluster...
################################################################################
INFO[0000] Using config file /home/czentye/PTX-edge-computing/kubernetes/test/manifests/k3d-test_cluster_multi.yaml (k3d.io/v1alpha5#simple) 
INFO[0000] portmapping '8080:80' targets the loadbalancer: defaulting to [servers:*:proxy agents:*:proxy] 
INFO[0000] Prep: Network                                
INFO[0000] Re-using existing network 'k3d-test-suite-cluster' (c3e409aada94529106f694668f107477b770aaf96050f7587a8592fe5065212a) 
INFO[0000] Created image volume k3d-test-suite-cluster-images 
INFO[0000] Starting new tools node...                   
INFO[0000] Starting node 'k3d-test-suite-cluster-tools' 
INFO[0001] Creating node 'k3d-test-suite-cluster-server-0' 
INFO[0001] Creating node 'k3d-test-suite-cluster-agent-0' 
INFO[0001] Creating node 'k3d-test-suite-cluster-agent-1' 
INFO[0001] Creating LoadBalancer 'k3d-test-suite-cluster-serverlb' 
INFO[0001] Using the k3d-tools node to gather environment information 
INFO[0001] HostIP: using network gateway 172.20.0.1 address 
INFO[0001] Starting cluster 'test-suite-cluster'        
INFO[0001] Starting servers...                          
INFO[0001] Starting node 'k3d-test-suite-cluster-server-0' 
INFO[0007] Starting agents...                           
INFO[0007] Starting node 'k3d-test-suite-cluster-agent-0' 
INFO[0007] Starting node 'k3d-test-suite-cluster-agent-1' 
INFO[0013] Starting helpers...                          
INFO[0013] Starting node 'k3d-test-suite-cluster-serverlb' 
INFO[0020] Injecting records for hostAliases (incl. host.k3d.internal) and for 4 network members into CoreDNS configmap... 
INFO[0022] Cluster 'test-suite-cluster' created successfully! 
INFO[0022] You can now use it like this:                
kubectl cluster-info

################################################################################
###   Create namespace...
################################################################################
namespace/ptx-edge created

testPolicyZoneSchedulingWithNodeAffinity

################################################################################
###   Create Privacy Zone restricted pod...
################################################################################
pod/pz-restricted-pod created
pod/pz-restricted-pod condition met
NAME                              STATUS   ROLES                  AGE   VERSION        ZONE-A   ZONE-B   ZONE-C
k3d-test-suite-cluster-agent-0    Ready    <none>                 25s   v1.31.5+k3s1   true     true     
k3d-test-suite-cluster-agent-1    Ready    <none>                 24s   v1.31.5+k3s1            true     
k3d-test-suite-cluster-server-0   Ready    control-plane,master   29s   v1.31.5+k3s1                     true
NAME                READY   STATUS    RESTARTS   AGE   IP          NODE                             NOMINATED NODE   READINESS GATES
pz-restricted-pod   1/1     Running   0          16s   10.42.0.4   k3d-test-suite-cluster-agent-0   <none>           <none>

Check node scheduling...
--------------------------------------------------------------------------------

Selected node: k3d-test-suite-cluster-agent-0

################################################################################
###   Delete resources...
################################################################################
pod "pz-restricted-pod" deleted
namespace "ptx-edge" deleted

################################################################################
###   Delete cluster...
################################################################################
INFO[0000] Deleting cluster 'test-suite-cluster'        
INFO[0003] Deleting 1 attached volumes...               
INFO[0003] Removing cluster details from default kubeconfig... 
INFO[0003] Removing standalone kubeconfig file (if there is one)... 
INFO[0003] Successfully deleted cluster test-suite-cluster! 

Ran 1 test.

OK
```

### Unit tests

Each subproject define it own test cases.
To install test dependencies of a given component, refer to the related `README` files.

Each subproject defines a `Makefile` to unify the development/test environment creation.
Accordingly, test environment configuration (and execution) is implicitly managed by
external tools and third-party libraries, such as
[virtualenv](https://virtualenv.pypa.io/en/latest/),
[pytest](https://docs.pytest.org/en/stable/), and
[tox](https://tox.wiki/en/4.24.1/), within these Makefiles.

To explicitly set up the test/dev environment for a `project` locally
(without Docker), the following command can be used:

```bash
$ cd kubernetes/src/<project> && make setup
```

Furthermore, the configuration of docker-based test environments can be also performed
explicitly by executing the dedicated `Makefile` target as follows:

```bash
$ cd kubernetes/src/<project> && make docker-test-setup # Preferred way
```

Similarly to component-level tests, to locally execute all unit tests defined
for `ptx-edge`, use the following helper script in `kubernetes/test/units`:

```bash
$ cd kubernetes/test/units && ./runall.sh
```

To locally execute the unit tests of a single `project`,
execute the dedicated Makefile target within the project folder, e.g.,

```bash
$ cd kubernetes/src/<project> && make unit-tests
```

Components define different dependencies and test parameters wrapped by their Makefiles.
Thus, the preferred way for testing is the preconfigured Docker-based test environments.

For docker-based test execution, use the dedicated `-d` flag of `runall.sh`
or call the dedicated `Makefile` target of any subproject:

```bash
$ cd kubernetes/test/units && ./runall.sh -d
# or
$ cd kubernetes/src/<project> && make docker-unit-tests
```

Test reports are automatically generated and saved during the tests.
To export the test report from all test containers, use the following `-o` flag:

```bash
$ ./runall.sh -d -o results/
```

## REST-API Mockup

The repository also provides a mockup REST-API under `mock-api` folder
for testing purposes based on its
[OpenAPI3.0 specification](mock-api/swagger_server/swagger/swagger.yaml).

The auto-generated mock API also provides test cases for the basic functionality
of the defined REST-API endpoints in [mock-api/swagger_server/test](mock-api/swagger_server/test).

- [test_default_controller.py](mock-api/swagger_server/test/test_default_controller.py):
  test cases of the default API (`/version`)
- [test_customer_api_controller.py](mock-api/swagger_server/test/test_customer_api_controller.py):
  test cases of the customer-facing API (`/requestEdgeProc`, `/requestPrivacyEdgeProc`)

Testing the endpoints can be performed using the following two approaches:

- Executing the automated unit test cases with ``tox`` or ``nosetests`` tools.
- Manual endpoint testing directly from the Swagger UI available on
  http://localhost:8080/ptx-edge/v1/ui/

> [!NOTE]
>
> Descriptions of mockup unit tests can be found in the related [README.md](mock-api/README.md).
